{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nif not os.path.isdir(\"./download_google_drive/\"):","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# We define which sequences the functions is going to show and which view-angles\nscondition = np.array(['nm-01','bg-01','cl-01'])\nsviews = np.array(['000','018','036','054','072','090','108','126','144','162','180'])  \n\ndef plot_subject(subject, image, sviews, scondition,name):\n  plt.figure(figsize=(14,4))\n  count = 1\n  for i, cond in enumerate(scondition):\n    for j, view in enumerate(sviews):\n      plt.subplot(len(scondition), len(sviews), count)\n      path = data_base+slash+data_set_rep+slash+subject+slash+cond+slash+view+slash+image+'.png'\n      plt.imshow(cv2.imread(path,0),cmap='gray')\n      plt.xticks([])\n      plt.yticks([])\n      if(j==0):\n        plt.ylabel(cond)\n      if(i==len(scondition)-1):\n        plt.xlabel(view)\n      count +=1\n  plt.subplots_adjust(wspace=0.05, hspace=0.01)\n  plt.savefig(name+'.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subject = '012'\nplot_subject(subject,'1',sviews,scondition,'Original_GEI')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom os import listdir\nimport cv2\nimport os\nfrom glob import glob\nimport time\nt0 = time.clock()\nimport matplotlib.pyplot as plt\n\ndata_base = 'CASIA'\ndata_set = 'DatasetB'\ndata_set_rep = 'sil'\ndirectorio = 'silhouettes'\ncondition = np.array(['bg-01','bg-02','cl-01','cl-02','nm-01','nm-02','nm-03','nm-04','nm-05','nm-06'])\nformato = '.png'\nvisualize = 0\nslash='/'\n\ncount = 1\n## Iterate through each subject\nfor subject in range(1,63):\n  print('\\rSubject:',subject,end='')\n  subject = str(subject).zfill(3)\n\n## Iterate through the normal conditions (Target images)\n  for dom_con in condition[4:]:\n    view = '090'\n    slash='/'\n    rep_sil = data_base+slash+data_set_rep+slash+subject+slash+dom_con+slash+view+slash+'1.png'\n    GEI_dom = cv2.imread(rep_sil,0)\n    \n## Iterate through each condition (Source images)\n    for i in range(len(condition)):\n  ## Iterate through view\n      for vi in range(11):\n        view = str(vi*18).zfill(3)\n        slash='/'\n        rep_sil = data_base+slash+data_set_rep+slash+subject+slash+condition[i]+slash+view+slash+'1.png'\n        GEI = cv2.imread(rep_sil,0)\n        \n        # Save the GEIs\n        inputdata = \"CASIA/pix2pix/inputdata/\"\n        targetdata = \"CASIA/pix2pix/targetdata/\"\n        cv2.imwrite(inputdata+'s'+str(subject).zfill(3)+'_'+str(count).zfill(5)+'.png',GEI)\n        cv2.imwrite(targetdata+'s'+str(subject).zfill(3)+'_'+str(count).zfill(5)+'.png',GEI_dom)\n\n        count +=1\n        if(visualize):\n          f = plt.figure()\n          f.add_subplot(1,2, 1)\n          plt.imshow(GEI, cmap='gray')\n          f.add_subplot(1,2, 2)\n          plt.imshow(GEI_target,cmap='gray')\n          plt.show(block=True)  \n        if cv2.waitKey(50) & 0xff==27:\n          break\n\nprint('\\tTime: {} s'.format(round((time.clock()-t0),2)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Paths\nPATH    = '/content/CASIA/pix2pix/'\ninpath  = '/content/CASIA/pix2pix/inputdata'\noutpath = '/content/CASIA/pix2pix/targetdata'\n\nimgurls = !ls -1 \"{inpath}\"\n\nnp.random.seed(23)\nnp.random.shuffle(imgurls)\n\n# imgurls = imgurls[:20000]\nn = len(imgurls)\ntrain_n = round(n*0.90)\n\n# Lista random\nrandurls = np.copy(imgurls)\n\ntr_urls = randurls[:train_n]\nts_urls = randurls[train_n:n]\n\nprint('Total images:',len(imgurls),' \\nTraining:',len(tr_urls),' Testing:', len(ts_urls))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_WIDTH = 64\nIMG_HEIGHT = 64\n\n\n# Resize images\ndef resize(inimg, tgimg, height, width):\n  inimg = tf.image.resize(inimg, [height, width])\n  tgimg = tf.image.resize(tgimg, [height, width])\n  return inimg, tgimg\n\n# Normalize the pixel's values\ndef normalize(inimg, tgimg):\n  inimg = (inimg/127) - 1\n  tgimg = (tgimg/127) - 1\n  return inimg, tgimg\n\n@tf.function()\ndef random_jitter(inimg, tgimg):\n  inimg, tgimg = resize(inimg, tgimg, int(IMG_WIDTH*1.02), int(IMG_HEIGHT*1.02))\n  stacked_image = tf.stack([inimg, tgimg], axis = 0)\n  cropped_image = tf.image.random_crop(stacked_image, size=[2,IMG_HEIGHT,IMG_WIDTH,1])\n  inimg, tgimg = cropped_image[0], cropped_image[1]\n  return inimg, tgimg\n\n@tf.function()\ndef load_image(filename, augment = True):\n  inimg = tf.image.rgb_to_grayscale(tf.image.decode_png(tf.io.read_file(inpath+'/'+filename),channels=3))\n  tgimg = tf.image.rgb_to_grayscale(tf.image.decode_png(tf.io.read_file(outpath+'/'+filename),channels=3))\n  \n  inimg, tgimg = resize(inimg,tgimg, IMG_HEIGHT, IMG_WIDTH)\n  \n  if augment:\n    inimg, tgimg = random_jitter(inimg, tgimg)\n    \n  inimg, tgimg = normalize(inimg, tgimg)\n  return inimg, tgimg\n  \ndef load_train_image(filename):\n  return load_image(filename, True)\n\ndef load_test_image(filename):\n  return load_image(filename, False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inimg, tgimg = load_train_image('s001_00001.png')\nplt.imshow(inimg.numpy()[:,:,0]*+1/2,cmap='gray')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices(tr_urls)\ntrain_dataset = train_dataset.map(load_train_image,num_parallel_calls=AUTOTUNE)\ntrain_dataset = train_dataset.batch(200)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(ts_urls)\ntest_dataset = test_dataset.map(load_test_image,num_parallel_calls=AUTOTUNE)\ntest_dataset = test_dataset.batch(200)\n\nfor inimg, tgimg in train_dataset.take(5):\n  f = plt.figure()\n  f.add_subplot(1,2, 1)\n  plt.title('Source image')\n  plt.imshow((inimg[0,...,0]+1)/2,cmap='gray')\n  f.add_subplot(1,2, 2)\n  plt.title('Target image')\n  plt.imshow((tgimg[0,...,0]+1)/2,cmap='gray')\n  plt.show(block=True) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import *\nfrom tensorflow.keras.layers import *\n\ndef downsample(filters, apply_batchnorm=True):\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(Conv2D(filters, \n                    kernel_size = 4, \n                    strides=2, \n                    padding='same',\n                    kernel_initializer=initializer, \n                    use_bias= not apply_batchnorm))\n\n  if apply_batchnorm:\n    result.add(BatchNormalization())\n\n#   result.add(LeakyReLU())\n  result.add(tf.keras.layers.LeakyReLU())\n\n  return result\n\ndef upsample(filters, apply_dropout=False):\n  initializer = tf.random_normal_initializer(0., 0.02)\n  result = tf.keras.Sequential()\n  result.add(Conv2DTranspose(filters, \n                             kernel_size = 4, \n                             strides=2, \n                             padding='same',\n                             kernel_initializer=initializer, \n                             use_bias= False))\n  result.add(BatchNormalization())\n  if apply_dropout:\n    result.add(Dropout(0.3))\n\n  result.add(ReLU())\n#   result.add(tf.keras.layers.LeakyReLU())\n\n  return result","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Generator():\n  inputs = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH,1])\n\n  down_stack = [\n    downsample(64, apply_batchnorm=False),                                      # (bs, 32, 32, 64)\n    downsample(128),                                                            # (bs, 16, 16, 128)\n    downsample(256),                                                            # (bs, 8,  4,  256)\n    downsample(512),                                                            # (bs, 4,  4,  512)\n  ]\n\n  up_stack = [\n    upsample(256, apply_dropout=True),                                          # (bs, 8,  8,  256)\n    upsample(128, apply_dropout=False),                                         # (bs, 16, 16, 128)\n    upsample(64, apply_dropout=False),                                          # (bs, 32, 32, 64)\n  ]\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n  last = tf.keras.layers.Conv2DTranspose(filters = 1, \n                                         kernel_size = 4,\n                                         strides=2,\n                                         padding='same',\n                                         kernel_initializer=initializer,\n                                         activation='tanh')                     # (bs, 64, 64, 1)\n\n  concat = tf.keras.layers.Concatenate()\n  x = inputs\n\n  # Downsampling through the model\n  skips = []\n  for down in down_stack:\n    x = down(x)\n    skips.append(x)\n\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    x = tf.keras.layers.Concatenate()([x, skip])\n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator = Generator()\ngen_output = generator((inimg), training=False)\nf = plt.figure()\nf.add_subplot(1,2, 1)\nplt.imshow((inimg[0,...,0]+1)/2,cmap='gray')\nf.add_subplot(1,2, 2)\nplt.imshow((gen_output[0,...,0]+1)/2,cmap='gray')\nplt.show(block=True)\ntf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Identificador_Discriminator():\n  initializer = tf.random_normal_initializer(0., 0.02)\n  inp = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 1], name='input_image')\n  tar = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 1], name='target_image')\n\n  x = tf.keras.layers.concatenate([inp, tar])                                   # (bs, 64, 64, 2)\n\n  down1 = downsample(64, False)(x)                                              # (bs, 32, 32, 64)\n  down2 = downsample(128)(down1)                                                # (bs, 16, 16, 128)\n  down3 = downsample(256)(down2)                                                # (bs, 8,  8,  256)\n  last = tf.keras.layers.Conv2D(filters=1,\n                                kernel_size = 4,\n                                strides=1,\n                                kernel_initializer=initializer,\n                                padding = 'valid',\n                                activation='sigmoid')(down3)                    # (bs, 5, 5, 1)\n  \n\n  return tf.keras.Model(inputs=[inp, tar], outputs=last)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idiscriminator= Identificador_Discriminator()\ndisc_out = idiscriminator([(inimg), gen_output], training=False)\nplt.figure(figsize=(4,4))\nplt.imshow(disc_out[0,...,0], vmin=0, vmax=1, cmap='RdBu_r')\nplt.colorbar()\nplt.xticks([])\nplt.yticks([])\nplt.show()\ndisc_out.shape\n\ntf.keras.utils.plot_model(idiscriminator, show_shapes=True, dpi=64)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef idiscriminator_loss(disc_real_output, disc_generated_output):\n  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n  total_disc_loss = real_loss + generated_loss\n  return total_disc_loss\n\nLAMBDA = 400\ndef generator_loss(idisc_generated_output, gen_output, target):\n  gan_loss1 = loss_object(tf.ones_like(idisc_generated_output), idisc_generated_output)\n  # mean absolute error\n  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n  total_gen_loss = gan_loss1 + (LAMBDA * l1_loss)\n  return total_gen_loss, gan_loss1, l1_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ngenerator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\nidiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndef generate_images(model, test_input, tar, save_filename = False, display_imgs = True):\n  prediction = model(test_input, training=True)\n  if save_filename:\n    tf.keras.preprocessing.image.save_img(PATH+'outputs/'+save_filename+'.jpg', prediction[0,...])\n  \n  plt.figure(figsize=(7,7))\n\n  display_list = [test_input[0,...,], tar[0,...,], prediction[0,...,]]\n  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n\n  for i in range(3):\n    plt.subplot(1, 3, i+1)\n    plt.title(title[i])\n    # getting the pixel values between [0, 1] to plot it.\n    plt.imshow(display_list[i][...,0] * 0.5 + 0.5, cmap = 'gray')\n    plt.axis('off')\n  plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\nlog_dir=\"logs/\"\n\nsummary_writer = tf.summary.create_file_writer(\n  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n@tf.function\ndef train_step(input_image, target, epoch):\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as idisc_tape, tf.GradientTape() as rfdisc_tape:\n    gen_output = generator(input_image, training=True)\n\n    idisc_real_output      = idiscriminator([input_image, target], training=True)\n    idisc_generated_output = idiscriminator([input_image, gen_output], training=True)\n\n    gen_loss,ganloss1,l1loss = generator_loss(idisc_generated_output, gen_output, target)\n    idisc_loss = idiscriminator_loss(idisc_real_output, idisc_generated_output)\n\n  generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n  idiscriminator_gradients = idisc_tape.gradient(idisc_loss, idiscriminator.trainable_variables)\n\n  generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n  idiscriminator_optimizer.apply_gradients(zip(idiscriminator_gradients, idiscriminator.trainable_variables))\n  \n  with summary_writer.as_default():\n    tf.summary.scalar('gen_total_loss', gen_loss, step=epoch)\n    tf.summary.scalar('gen_gan_loss', ganloss1, step=epoch)\n    tf.summary.scalar('gen_l1_loss', l1loss, step=epoch)\n    tf.summary.scalar('idisc_loss', idisc_loss, step=epoch)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\nimport time\n\ndef fit(train_ds, epochs, test_ds):\n  for epoch in range(epochs):\n    start = time.time()\n\n    imgi = 0\n    for input_image, target in train_ds:\n      print('\\r','Epoch: '+str(epoch)+' - train: '+str(imgi)+'/'+str(len(tr_urls)),end='')\n      imgi +=1\n      train_step(input_image, target, epoch)\n\n    imgi = 0\n    for example_input, example_target in test_ds.take(3):\n      generate_images(generator, example_input, example_target, str(imgi)+'_'+str(epoch), display_imgs=True)\n      imgi +=1\n\n    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                        time.time()-start))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\nfit(train_dataset, EPOCHS, test_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nidrange = [63,125]\ndef transf(name):\n  img = tf.image.rgb_to_grayscale(tf.image.resize((tf.image.decode_png(tf.io.read_file(name),channels=3)),(64, 64)))\n  img = img[np.newaxis]/127-1\n  pred = np.array(255*((generator(img,training = False)[0]+1)/2))\n  return pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom numpy import ma\nfrom os import listdir\nimport cv2\nimport os\nfrom glob import glob\nimport time\nt0 = time.clock()\n\ndata_base = 'CASIA'\ndata_set = 'DatasetB' \ndata_set_rep = 'sil'\ndata_train = 'data'\ndirectorio = 'silhouettes'\t\n\ntrain_   = np.array(['nm-01','nm-02','nm-03','nm-04'])\ntest_nm_ = np.array(['nm-05','nm-06'])\ntest_cl_ = np.array(['cl-01','cl-02'])\ntest_bg_ = np.array(['bg-01','bg-02'])\nview = '090'\t\nformato = '.png'\n\n\ndata = {}\ndata[\"train_\"] = train_\ndata[\"test_nm_\"] = test_nm_\ndata[\"test_cl_\"] = test_cl_\ndata[\"test_bg_\"] = test_bg_\n\ndef transform_data(verbose=True):\n  for subject in range(idrange[0],idrange[1]):\n    subject = str(subject).zfill(3)\n    if verbose:\n      print('\\r', 'Subject: ',subject, end='')\n    for dset in data:\n      condition = data[dset]\n      for cond in condition:\n        for view in range(11):\n          view = str(view*18).zfill(3)\n          GEI_path = data_base+slash+data_set_rep+slash+subject+slash+cond+slash+view+slash+'1.png'                 #CASIA/sil/104/nm-04/090/1.png\n          GEI_save_path = data_base+slash+data_set_rep+slash+subject+slash+cond+slash+view+slash+file_name+'.png'   #CASIA/sil/104/nm-04/090/4.png\n          GEI = transf(GEI_path)\n          cv2.imwrite(GEI_save_path,GEI)\n  if verbose:\n    print(\"\\tTransform_data: \",str(time.clock()-t0))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom numpy import ma\nfrom os import listdir\nimport cv2\nimport os\nfrom glob import glob\nimport time\nt0 = time.clock()\n\ndata_base = 'CASIA'\ndata_set = 'DatasetB' \ndata_set_rep = 'sil'\ndata_train = 'data'\ndirectorio = 'silhouettes'\t\nviews = np.array(['000','018','036','054','072','090','108','126','144','162','180'])\ntrain_ = np.array(['nm-01','nm-02','nm-03','nm-04'])\ntest_nm_ = np.array(['nm-05','nm-06'])\ntest_cl_ = np.array(['cl-01','cl-02'])\ntest_bg_ = np.array(['bg-01','bg-02'])\nformato = '.png'\n\n\ndata = {}\ndata[\"train_\"] = train_\ndata[\"test_nm_\"] = test_nm_\ndata[\"test_cl_\"] = test_cl_\ndata[\"test_bg_\"] = test_bg_\n\n\ndef split_flat(verbose=True):\n  t0 = time.clock()\n  for view in views:\n    if verbose: \n      print('View: ', str(view))\n  #   view = str(view*18).zfill(3)\n    for dset in data:\n      if verbose: \n        print(dset, ':',data[dset])\n      condition = data[dset]\n      matriz  = []\n      etiqueta = []\n      c1 =0\n      for subject in range(idrange[0], idrange[1]):\n        subject = str(subject).zfill(3)\n  #       print('Subject: ', subject)\n        for i in range(len(condition)):\n          # Definimos el nombre de la imagen\n          read_gei = data_base+slash+data_set_rep+slash+subject+slash+condition[i]+slash+view+slash+file_name+'.png'\n          img = cv2.imread(read_gei,0)\n  #         img[int(x1*0.12):int(x1*0.68),0:y1]=0\n\n          try:\n            matriz.append(img.flatten())\n            etiqueta.append(int(subject))\n          except:\n            print('Error saving:', read_gei)\n\n          if cv2.waitKey(1) & 0xff==27:\n            break\n\n      matriz = np.array(matriz)\n      etiqueta = np.array(etiqueta)[np.newaxis].T\n      directory_data = data_base+slash+data_train+slash+view+slash\n      if not os.path.exists(directory_data):\n          os.makedirs(directory_data)\n      np.savetxt(directory_data+dset+'data.dat', matriz)\n      np.savetxt(directory_data+dset+'target.dat', etiqueta)\n      if verbose:\n        print(dset,'data.dat-> ',matriz.shape)\n        print(dset,'data.dat-> ',etiqueta.shape)\n\n  cv2.destroyAllWindows()\n  if verbose:\n    print(str(time.clock()-t0))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import svm\n\n\ndef normalizacion(data):\n\tu = np.mean(data,axis=0)\n\ts = np.std(data,axis=0)\n\tdata = (data - u)/s\n\treturn (data,u,s)\ndef normalizacion2(data,u,s):\n\tdata = (data - u) / s\n\treturn data\n\ndatasets = ['test_nm_','test_bg_','test_cl_']\ndata_base = 'CASIA'\ndata_train = 'data'\nslash = '/'\n\nCCR_nm = np.zeros((11,11))\nCCR_bg = np.zeros((11,11))\nCCR_cl = np.zeros((11,11))\n\ndef test_model(iteration):\n\n\n\n  for view_g,galery_view in enumerate(views):\n    # Cargamos la base de datos\n    carpeta = data_base+slash+data_train+slash+galery_view+slash\n\n    trainX = np.loadtxt(carpeta+'train_data.dat')\n\n    # Escalamos los datos\n    scaler = StandardScaler()\n    scaler.fit(trainX)\n    trainX = scaler.transform(trainX)\n\n    trainY = np.loadtxt(carpeta+'train_target.dat')\n\n    _ ,componentes_original = trainX.shape\n\n    # Aplicamos PCA\n    pre = 0.9999\n    pca = PCA(pre)\n    pca.fit(trainX)\n    componentes_PCA = pca.n_components_\n    trainX = pca.transform(trainX)\n\n    logisticRegr = LinearDiscriminantAnalysis(solver = 'lsqr',shrinkage=0.2)\n    logisticRegr.fit(trainX, trainY)\n\n\n    # Calculamos su score\n    score = logisticRegr.score(trainX, trainY)\n    print('Dataset: Train',' shape test: ',trainX.shape,' correcto: ',np.round(score,4), 'Angle:', galery_view)\n    # Realizamos pruebas en los datasets de testeo\n    for view_p,prove_view in enumerate(views):\n      carpeta_p = data_base+slash+data_train+slash+prove_view+slash\n      for i in range(len(datasets)):\n        testX = np.loadtxt(carpeta_p+datasets[i]+'data.dat')\n        testX = scaler.transform(testX)\n        # testX = normalizacion2(testX,mean,st)\n        testX = pca.transform(testX)\n        testY = np.loadtxt(carpeta_p+datasets[i]+'target.dat')\n\n        # Calculamos su score\n        score = logisticRegr.score(testX, testY)\n        certeza = logisticRegr.predict(testX)\n\n        # Mostramos la matriz de confusion\n        # print(confusion_matrix(certeza,testY))\n        if(i==0):\n          CCR_nm[view_g,view_p] = np.round(score*100,2)\n        if(i==1):\n          CCR_bg[view_g,view_p] = np.round(score*100,2)\n        if(i==2):\n          CCR_cl[view_g,view_p] = np.round(score*100,2)\n\n      print('Prove view:', prove_view, '\\tnm: ',CCR_nm[view_g,view_p],'\\tbg: ',CCR_bg[view_g,view_p],'\\tcl: ',CCR_cl[view_g,view_p])\n\n\n  print(str(iteration)+'T Test:1-62',np.round(CCR_nm.mean(),2),'-',np.round(CCR_bg.mean(),2),'-',np.round(CCR_cl.mean(),2))\n  print(str(iteration)+'T Test:63-124',np.round(CCR_nm.mean(),2),'-',np.round(CCR_bg.mean(),2),'-',np.round","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_name='2'\ntransform_data(verbose=True)\nsplit_flat(verbose=False)\ntest_model('30')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nprint('\\t    | NORMAL WALKING | ROWS = GALLERY | COLUMNS = PROVE |')\npd.DataFrame(data=CCR_nm, columns=views, index=views)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\t    | CARRYING WALKING | ROWS = GALLERY | COLUMNS = PROVE |')\npd.DataFrame(data=CCR_bg, columns=views, index=views)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\t    | CLOTHING WALKING | ROWS = GALLERY | COLUMNS = PROVE |')\npd.DataFrame(data=CCR_cl, columns=views, index=views)","metadata":{},"execution_count":null,"outputs":[]}]}